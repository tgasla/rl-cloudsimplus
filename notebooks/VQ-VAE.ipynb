{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rich.progress import Progress\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        # Initialize the codebook (embedding vectors)\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten input to (batch_size * num_features, embedding_dim)\n",
    "        flat_x = x.view(-1, self.embedding_dim)\n",
    "\n",
    "        # Compute distances between input and embedding vectors\n",
    "        distances = torch.cdist(flat_x, self.embedding.weight, p=2)  # L2 distance\n",
    "        indices = torch.argmin(distances, dim=1)  # Closest embedding index\n",
    "\n",
    "        # Get quantized vectors\n",
    "        quantized = self.embedding(indices).view(x.shape)\n",
    "\n",
    "        # Get the unique values and create a mapping\n",
    "        unique_vals, inverse_indices = torch.unique(\n",
    "            quantized, sorted=True, return_inverse=True\n",
    "        )\n",
    "\n",
    "        # # Map each unique value to an integer from 0 to num_unique - 1\n",
    "        # integer_mapped_tensor = inverse_indices.view(quantized.shape)\n",
    "        # quantized = integer_mapped_tensor.float()\n",
    "\n",
    "        # Compute commitment loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Add quantization noise during backward pass\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        return quantized, indices, loss\n",
    "\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        latent_dim,\n",
    "        num_embeddings,\n",
    "        dropout=0.0,\n",
    "        use_batch_norm=False,\n",
    "        commitment_cost=0.25,\n",
    "    ):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128) if use_batch_norm else nn.Identity(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, latent_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.quantizer = VectorQuantizer(num_embeddings, latent_dim, commitment_cost)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128) if use_batch_norm else nn.Identity(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid(),  # Assuming input values are normalized to [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        latent = self.encoder(x)\n",
    "        # Quantize\n",
    "        quantized, indices, quantization_loss = self.quantizer(latent)\n",
    "        # Decode\n",
    "        reconstructed = self.decoder(quantized)\n",
    "        return latent, quantized, reconstructed, quantization_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class to handle CSV input and padding\n",
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, csv_file, padding_length=161):\n",
    "        # Load data from CSV\n",
    "        self.padding_length = padding_length\n",
    "        self.data = self._load_csv(csv_file)\n",
    "\n",
    "    def _min_max_normalize(self, array, min_val=0, max_val=100):\n",
    "        \"\"\"\n",
    "        Normalize array to the range [0, 1] based on given min_val and max_val.\n",
    "        \"\"\"\n",
    "        array = np.array(array, dtype=np.float32)\n",
    "        return (array - min_val) / (max_val - min_val)\n",
    "\n",
    "    def _load_csv(self, csv_file):\n",
    "        \"\"\"Read CSV file using Python's built-in csv module.\"\"\"\n",
    "        data = []\n",
    "        with open(csv_file, \"r\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            with Progress() as progress:\n",
    "                task = progress.add_task(\n",
    "                    \"[cyan]Processing CSV...\", total=sum(1 for _ in csvfile)\n",
    "                )  # Total rows in file\n",
    "                csvfile.seek(0)  # Reset file pointer\n",
    "\n",
    "                for row in reader:\n",
    "                    row = list(map(int, row))\n",
    "                    normalized_row = self._min_max_normalize(row)\n",
    "                    data.append(normalized_row)\n",
    "                    # Update progress bar\n",
    "                    progress.update(task, advance=1)\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the dataset (number of rows).\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieve a single data point from the dataset.\"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_for_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # torch.backends.cudnn.enabled = False\n",
    "\n",
    "    # Limit NumPy threads\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "    os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs, train_loader, device):\n",
    "    print(f\"Training the Autoencoder, Total epochs: {epochs}\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_reconstruction_loss = 0\n",
    "        epoch_quantization_loss = 0\n",
    "        epoch_total_loss = 0\n",
    "        epoch_rmse = 0\n",
    "        total_samples = 0\n",
    "        for batch in tqdm(\n",
    "            train_loader, desc=f\"Epoch [{epoch+1}/{epochs}]\", unit=\"batch\"\n",
    "        ):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            latent, quantized, reconstructed, quantization_loss = model(batch)\n",
    "            reconstruction_loss = F.mse_loss(\n",
    "                reconstructed, batch\n",
    "            )  # Reconstruction loss\n",
    "            rmse = torch.sqrt(reconstruction_loss)\n",
    "            total_loss = reconstruction_loss + quantization_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            unique_values, counts = torch.unique(quantized, return_counts=True)\n",
    "\n",
    "            # Accumulate loss\n",
    "            batch_size = batch.size(0)\n",
    "            total_samples += batch_size\n",
    "\n",
    "            epoch_reconstruction_loss += reconstruction_loss.item() * batch_size\n",
    "            epoch_quantization_loss += quantization_loss.item() * batch_size\n",
    "            epoch_total_loss += total_loss.item() * batch_size\n",
    "            epoch_rmse += rmse.item() * batch_size\n",
    "            # Calculate and accumulate RMSE\n",
    "\n",
    "        # Compute average loss and RMSE over all samples\n",
    "        epoch_total_loss /= total_samples\n",
    "        epoch_reconstruction_loss /= total_samples\n",
    "        epoch_quantization_loss /= total_samples\n",
    "        epoch_rmse /= total_samples\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}], Reconstruction Loss: {epoch_reconstruction_loss:.6f}, \"\n",
    "            f\"Quantization Loss: {epoch_quantization_loss:.6f}, Total Loss: {epoch_total_loss:.6f}, RMSE: {epoch_rmse:.6f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Validate the VQ-VAE model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model: The VQ-VAE model.\n",
    "        val_loader: DataLoader for validation data.\n",
    "\n",
    "    Returns:\n",
    "        val_total_loss: Average total loss over the validation set.\n",
    "        val_reconstruction_loss: Average reconstruction loss.\n",
    "        val_quantization_loss: Average quantization loss.\n",
    "        val_rmse: Root Mean Squared Error (RMSE) based on reconstruction loss.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_reconstruction_loss = 0.0\n",
    "    val_quantization_loss = 0.0\n",
    "    val_total_loss = 0.0\n",
    "    val_rmse = 0.0\n",
    "    total_samples = 0\n",
    "    latent_representations = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in tqdm(val_loader, desc=\"Validating...\", unit=\"batch\"):\n",
    "            batch = batch.to(device)\n",
    "            # Move data to appropriate device\n",
    "            x = batch.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "            # Forward pass\n",
    "            latent, quantized, reconstructed, quantization_loss = model(x)\n",
    "            # Compute reconstruction loss and RMSE\n",
    "            reconstruction_loss = F.mse_loss(reconstructed, x)\n",
    "            rmse = torch.sqrt(reconstruction_loss)\n",
    "\n",
    "            # Compute total loss\n",
    "            total_loss = reconstruction_loss + quantization_loss\n",
    "\n",
    "            # Accumulate losses weighted by batch size\n",
    "            batch_size = x.size(0)\n",
    "            total_samples += batch_size\n",
    "            val_reconstruction_loss += reconstruction_loss.item() * batch_size\n",
    "            val_quantization_loss += quantization_loss.item() * batch_size\n",
    "            val_total_loss += total_loss.item() * batch_size\n",
    "            val_rmse += rmse.item() * batch_size\n",
    "\n",
    "            # Store latent representations\n",
    "            latent_representations.append(latent)\n",
    "\n",
    "    # Compute average losses and RMSE over all samples\n",
    "    val_reconstruction_loss /= total_samples\n",
    "    val_quantization_loss /= total_samples\n",
    "    val_total_loss /= total_samples\n",
    "    val_rmse /= total_samples\n",
    "\n",
    "    # Concatenate latent representations\n",
    "    latent_representations = torch.cat(latent_representations).cpu().numpy()\n",
    "\n",
    "    # Print validation metrics\n",
    "    print(f\"Latent representations shape: {latent_representations.shape}\")\n",
    "    print(\n",
    "        f\"Validation Reconstruction Loss: {val_reconstruction_loss:.6f}, \"\n",
    "        f\"Validation Quantization Loss: {val_quantization_loss:.6f}, \"\n",
    "        f\"Validation Total Loss: {val_total_loss:.6f}, \"\n",
    "        f\"Validation RMSE: {val_rmse:.6f}\"\n",
    "    )\n",
    "\n",
    "    return val_rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6763ed09a8f74aafb7690bf97c10e253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 851229\n",
      "Train size: 680983\n",
      "Validation size: 170246\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "seed = 1234\n",
    "set_seed_for_everything(seed=seed)\n",
    "batch_size = 32\n",
    "\n",
    "# Load dataset\n",
    "csv_file = \"data/train_data.csv\"  # Replace with your CSV file path\n",
    "dataset = TreeDataset(csv_file)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "train_size = int(0.8 * len(dataset))\n",
    "print(f\"Train size: {train_size}\")\n",
    "val_size = len(dataset) - train_size\n",
    "print(f\"Validation size: {val_size}\")\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=lambda worker_id: np.random.seed(seed),\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    worker_init_fn=lambda worker_id: np.random.seed(seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model, optimizer, and number of epochs\n",
    "input_dim = 161  # Example input dimension (e.g., flattened 28x28 images)\n",
    "latent_dim = 64  # Latent dimension\n",
    "num_embeddings = 64  # Number of discrete codes\n",
    "epochs = 10  # Number of epochs\n",
    "lr = 0.0001\n",
    "dropout = 0\n",
    "weight_decay = 0\n",
    "\n",
    "model = VQVAE(\n",
    "    input_dim, latent_dim, num_embeddings, dropout=dropout, use_batch_norm=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Autoencoder, Total epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|██████████| 21281/21281 [00:26<00:00, 814.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Reconstruction Loss: 0.006435, Quantization Loss: 0.015624, Total Loss: 0.022059, RMSE: 0.031765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|██████████| 21281/21281 [00:26<00:00, 808.48batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Reconstruction Loss: 0.000075, Quantization Loss: 0.000207, Total Loss: 0.000282, RMSE: 0.008575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|██████████| 21281/21281 [00:26<00:00, 808.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Reconstruction Loss: 0.000072, Quantization Loss: 0.000219, Total Loss: 0.000291, RMSE: 0.008431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|██████████| 21281/21281 [00:25<00:00, 821.94batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Reconstruction Loss: 0.000071, Quantization Loss: 0.000224, Total Loss: 0.000295, RMSE: 0.008371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|██████████| 21281/21281 [00:26<00:00, 808.65batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Reconstruction Loss: 0.000071, Quantization Loss: 0.000225, Total Loss: 0.000296, RMSE: 0.008332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|██████████| 21281/21281 [00:26<00:00, 816.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Reconstruction Loss: 0.000070, Quantization Loss: 0.000223, Total Loss: 0.000294, RMSE: 0.008305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|██████████| 21281/21281 [00:26<00:00, 807.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Reconstruction Loss: 0.000070, Quantization Loss: 0.000220, Total Loss: 0.000290, RMSE: 0.008288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|██████████| 21281/21281 [00:26<00:00, 815.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Reconstruction Loss: 0.000070, Quantization Loss: 0.000218, Total Loss: 0.000288, RMSE: 0.008276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|██████████| 21281/21281 [00:26<00:00, 812.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Reconstruction Loss: 0.000070, Quantization Loss: 0.000221, Total Loss: 0.000290, RMSE: 0.008270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████████| 21281/21281 [00:26<00:00, 812.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Reconstruction Loss: 0.000069, Quantization Loss: 0.000221, Total Loss: 0.000290, RMSE: 0.008256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating...: 100%|██████████| 5321/5321 [00:02<00:00, 1775.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent representations shape: (170246, 64)\n",
      "Validation Reconstruction Loss: 0.000065, Validation Quantization Loss: 0.000201, Validation Total Loss: 0.000265, Validation RMSE: 0.007978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, epochs, train_loader, device)\n",
    "val_rmse = validate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"VQVAE_64dim_BN_64n.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
