{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader  # Corrected import for DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "from rich.progress import Progress  # Importing Progress from rich\n",
    "from tqdm import tqdm  # Importing tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "# class TreeGNN(torch.nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "#         super(TreeGNN, self).__init__()\n",
    "#         # GNN layers\n",
    "#         self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "#         self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "#         # Fully connected layer for the global embedding\n",
    "#         self.fc = Linear(hidden_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # GNN layers\n",
    "#         out = self.conv1(x, edge_index)\n",
    "#         out = F.relu(out)\n",
    "#         out = self.conv2(out, edge_index)\n",
    "#         # Global pooling to create a single embedding\n",
    "#         embedding = global_mean_pool(out, batch)\n",
    "#         # Final embedding\n",
    "#         reconstruction = self.fc(out)\n",
    "#         return embedding, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TreeGNN, self).__init__()\n",
    "        # GNN layers for encoding\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Linear layer for embedding generation\n",
    "        self.fc_embedding = Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Decoding layers for reconstruction\n",
    "        self.fc_decode1 = Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_decode2 = Linear(hidden_dim, input_dim)  # Match input_dim here\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Encoding\n",
    "        out = self.conv1(x, edge_index)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out, edge_index)\n",
    "\n",
    "        # Global pooling for graph-level embedding\n",
    "        embedding = global_mean_pool(out, batch)\n",
    "        embedding = self.fc_embedding(embedding)  # Final embedding\n",
    "\n",
    "        # Decoding for reconstruction\n",
    "        decoded = F.relu(self.fc_decode1(out))\n",
    "        reconstruction = self.fc_decode2(decoded)  # Output matches input dimension\n",
    "\n",
    "        return embedding, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(array, min_val=0, max_val=100):\n",
    "    \"\"\"\n",
    "    Normalize array to the range [0, 1] based on given min_val and max_val.\n",
    "    \"\"\"\n",
    "    array = np.array(array, dtype=np.float32)\n",
    "    return (array - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def read_and_process_csv(file_path, max_nodes=161, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Read the CSV file without using pandas, pad the arrays to max_nodes, and convert to graph format.\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    with open(file_path, \"r\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "\n",
    "        # Initialize rich progress bar\n",
    "        with Progress() as progress:\n",
    "            task = progress.add_task(\n",
    "                \"[cyan]Processing CSV...\", total=sum(1 for _ in csvfile)\n",
    "            )  # Total is calculated by counting rows\n",
    "            csvfile.seek(0)  # Reset file pointer to start of the file\n",
    "\n",
    "            # Read the file line by line\n",
    "            for row in csvreader:\n",
    "                row = list(map(int, row))\n",
    "                # Normalize the padded row\n",
    "                normalized_row = min_max_normalize(row, min_val=0, max_val=100)\n",
    "\n",
    "                # Create node features (each node has a single feature, its value)\n",
    "                node_features = torch.tensor(\n",
    "                    normalized_row, dtype=torch.float, device=device\n",
    "                ).view(-1, 1)\n",
    "\n",
    "                # Create edge index for a binary tree structure\n",
    "                edge_index = []\n",
    "                for i in range(1, max_nodes):\n",
    "                    parent = (i - 1) // 2\n",
    "                    edge_index.append([parent, i])  # Parent to child\n",
    "                    edge_index.append(\n",
    "                        [i, parent]\n",
    "                    )  # Child to parent (for undirected graph)\n",
    "\n",
    "                edge_index = (\n",
    "                    torch.tensor(edge_index, dtype=torch.long, device=device)\n",
    "                    .t()\n",
    "                    .contiguous()\n",
    "                )\n",
    "\n",
    "                # Single graph, so batch index is all zeros\n",
    "                batch = torch.zeros(max_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "                # Create a PyTorch Geometric Data object\n",
    "                graph = Data(x=node_features, edge_index=edge_index, batch=batch)\n",
    "                graphs.append(graph)\n",
    "\n",
    "                # Update progress bar\n",
    "                progress.update(task, advance=1)\n",
    "\n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_for_everything(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0046b8752a4e46e5b73ba8f078b24d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_seed_for_everything(1234)\n",
    "\n",
    "# Define the model\n",
    "input_dim = 1  # Dimension of node features\n",
    "hidden_dim = 64  # Hidden dimension in the GNN layers\n",
    "output_dim = 16  # Desired embedding size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = TreeGNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# File path to the CSV file\n",
    "file_path = \"train_data.csv\"  # Replace with your CSV file path\n",
    "\n",
    "# Process the CSV and create a dataset of graphs\n",
    "graphs = read_and_process_csv(file_path, device=device)\n",
    "\n",
    "# Split the dataset into 80% train and 20% validation\n",
    "train_size = int(0.8 * len(graphs))\n",
    "val_size = len(graphs) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    graphs, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Use DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 21281/21281 [01:00<00:00, 350.07batch/s]\n",
      "Epoch 2/10: 100%|██████████| 21281/21281 [01:00<00:00, 351.96batch/s]\n",
      "Epoch 3/10: 100%|██████████| 21281/21281 [01:01<00:00, 345.52batch/s]\n",
      "Epoch 4/10: 100%|██████████| 21281/21281 [01:02<00:00, 338.44batch/s]\n",
      "Epoch 5/10: 100%|██████████| 21281/21281 [01:01<00:00, 348.52batch/s]\n",
      "Epoch 6/10: 100%|██████████| 21281/21281 [01:01<00:00, 343.55batch/s]\n",
      "Epoch 7/10: 100%|██████████| 21281/21281 [01:02<00:00, 341.31batch/s]\n",
      "Epoch 8/10: 100%|██████████| 21281/21281 [01:03<00:00, 337.00batch/s]\n",
      "Epoch 9/10: 100%|██████████| 21281/21281 [01:00<00:00, 353.51batch/s]\n",
      "Epoch 10/10: 100%|██████████| 21281/21281 [01:00<00:00, 350.26batch/s]\n"
     ]
    }
   ],
   "source": [
    "# Training loop with progress bar\n",
    "epochs = 10  # Number of epochs\n",
    "lr = 0.01\n",
    "total_samples = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):  # Loop over epochs\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0  # To accumulate the loss for the epoch\n",
    "\n",
    "    # Create the tqdm progress bar for batches\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "        optimizer.zero_grad()  # Clear the gradients\n",
    "        embedding, reconstruction = model(\n",
    "            batch.x, batch.edge_index, batch.batch\n",
    "        )  # Forward pass\n",
    "        loss = F.mse_loss(reconstruction, batch.x)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the model parameters\n",
    "\n",
    "        # Update progress bar with the loss value\n",
    "        epoch_loss += loss.item()\n",
    "        batch_size = batch.x.size(0)\n",
    "        total_samples += batch_size\n",
    "\n",
    "        epoch_loss /= total_samples\n",
    "        epoch_rmse = torch.sqrt(torch.tensor(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.16176828063908e-07\n"
     ]
    }
   ],
   "source": [
    "print(epoch_rmse.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "torch.cuda.empty_cache()\n",
    "del model\n",
    "del graphs\n",
    "del train_dataset\n",
    "del val_dataset\n",
    "del train_loader\n",
    "del val_loader\n",
    "del optimizer\n",
    "del embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
