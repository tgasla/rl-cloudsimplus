# train_model_dir: applicable only if mode == [transfer|test]
# vm_allocation_policy: [random, roundrobin, worstfit, bestfit, firstfit]
common:
    timestep_interval: 1.0
    initial_s_vm_count: 0
    initial_m_vm_count: 0
    initial_l_vm_count: 0
    max_episode_length: 100
    vm_startup_delay: 0.0
    vm_shutdown_delay: 0.0
    medium_vm_multiplier: 2
    large_vm_multiplier: 4
    split_large_jobs: true
    clear_created_lists: true
    host_ram: 65536
    host_storage: 100000
    host_bw: 50000
    small_vm_ram: 8192
    small_vm_storage: 4000
    small_vm_bw: 1000
    paying_for_the_full_hour: false
    vm_hourly_cost: 0.086 #unused
    log_experiment: true

experiment_1:
    state_representation: treearray
    mode: train
    vm_allocation_policy: fromfile
    algorithm: strategy.csv
    host_count: 10
    host_pes: 10
    host_pe_mips: 10
    max_job_pes: 1
    small_vm_pes: 2
    job_trace_filename: three_peaks_max30
    timesteps: 1000000
    reward_job_wait_coef: 0.33
    reward_running_vm_cores_coef: 0.33
    reward_unutilized_vm_cores_coef: 0.33
    reward_invalid_coef: 0.0
    # train_model_dir: 240918-185051_PPO_2M_10H_10P_10M_three_peaks_max30_1MJC_0.25Q_0.25R_0.25U_0.25I_train_1fdff7236cd4

# experiment_2:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: worstfit
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: three_peaks_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_3:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: roundrobin
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: three_peaks_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_4:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: random
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: three_peaks_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_5:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: firstfit
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: three_peaks_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_6:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: rl
#     algorithm: PPO
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: three_peaks_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.25
#     reward_running_vm_cores_coef: 0.25
#     reward_unutilized_vm_cores_coef: 0.25
#     reward_invalid_coef: 0.25