# train_model_dir: applicable only if mode == [transfer|test]
# globals.log_destination: [none, stdout, file, stdout-file] - if file, logs are written to a file, if stdout-file, logs are written to both stdout and a file
# globals.junit_output_show: [true, false] - if true, junit test results are printed to stdout
# globals.log_level: [TRACE, DEBUG, INFO, WARNING, ERROR]
# seed: random or any integer
# base_log_dir: parent directory for all logs
# experiment_type_dir: parent directory for a set of experiments
# experiment_name: name of the experiment
# mode: [train, test, transfer]
# vm_allocation_policy: [rl, heuristic]
# algorithm: [any stable-baselines3 algorithm if vm_allocation_policy == rl, bestfit|firstfit|worstfit|roundrobin|random if vm_allocation_policy == heuristic]

# if you make any changes here, you need to do make build-gateway for the changes to take effect
globals:
    attached: true
    gpu: true
    java_log_level: INFO
    java_log_destination: none
    junit_output_show: false

common:
    timestep_interval: 1.0
    initial_s_vm_count: 0
    initial_m_vm_count: 0
    initial_l_vm_count: 0
    max_episode_length: 100
    vm_startup_delay: 0.0
    vm_shutdown_delay: 0.0
    medium_vm_multiplier: 2
    large_vm_multiplier: 4
    split_large_jobs: true
    clear_created_lists: true
    host_ram: 65536
    host_storage: 100000
    host_bw: 50000
    small_vm_ram: 8192
    small_vm_storage: 4000
    small_vm_bw: 1000
    paying_for_the_full_hour: false
    small_vm_hourly_cost: 0.086
    save_experiment: true
    base_log_dir: logs
    experiment_type_dir: test_if_state_dict_is_correct
    seed: random

experiment_1:
    state_as_tree_array: true
    state_as_dict: true
    job_queue_visible: true
    mode: train
    vm_allocation_policy: rl
    algorithm: PPO
    host_count: 10
    host_pes: 10
    host_pe_mips: 10
    max_job_pes: 1
    small_vm_pes: 2
    job_trace_filename: three_peaks_max30
    timesteps: 1000000
    reward_job_wait_coef: 0.25
    reward_running_vm_cores_coef: 0.25
    reward_unutilized_vm_cores_coef: 0.25
    reward_invalid_coef: 0.25
    experiment_name: test
    # train_model_dir: valley_window_evaluate_reward/10M

# experiment_2:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: rl
#     algorithm: PPO
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: valley_max30_window_10
#     timesteps: 1000000
#     reward_job_wait_coef: 0.25
#     reward_running_vm_cores_coef: 0.25
#     reward_unutilized_vm_cores_coef: 0.25
#     reward_invalid_coef: 0.25

# experiment_3:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: rl
#     algorithm: PPO
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: valley_max30_window_20
#     timesteps: 1000000
#     reward_job_wait_coef: 0.25
#     reward_running_vm_cores_coef: 0.25
#     reward_unutilized_vm_cores_coef: 0.25
#     reward_invalid_coef: 0.25

# experiment_4:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: rl
#     algorithm: PPO
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: valley_max30_window_50
#     timesteps: 1000000
#     reward_job_wait_coef: 0.25
#     reward_running_vm_cores_coef: 0.25
#     reward_unutilized_vm_cores_coef: 0.25
#     reward_invalid_coef: 0.25

# experiment_5:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: bestfit
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: two_peaks_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_6:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: bestfit
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: descending_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_7:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: bestfit
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: ascending_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_8:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: bestfit
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: hill_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0

# experiment_9:
#     state_representation: treearray
#     mode: train
#     vm_allocation_policy: heuristic
#     algorithm: bestfit
#     host_count: 10
#     host_pes: 10
#     host_pe_mips: 10
#     max_job_pes: 1
#     small_vm_pes: 2
#     job_trace_filename: valley_max30
#     timesteps: 1000000
#     reward_job_wait_coef: 0.33
#     reward_running_vm_cores_coef: 0.33
#     reward_unutilized_vm_cores_coef: 0.33
#     reward_invalid_coef: 0.0
