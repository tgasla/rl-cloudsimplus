# RL stuff
ALGO=PPO
# PRETRAIN_ENV=SmallDC-v0
# if PRETRAIN_DIR is not empty, then pretarining is not done and instead it is loaded from file
# so all other pretraining parameters are ignored
PRETRAIN_DIR=
PRETRAIN_TIMESTEPS=20000000
PRETRAIN_HOSTS=10
PRETRAIN_HOST_PES=10
PRETRAIN_HOST_PE_MIPS=10

# max pes per job cannot be greater than the largest VM's cores
PRETRAIN_MAX_PES_PER_JOB=1
PRETRAIN_JOB_TRACE_FILENAME=100jobs_1lambda_1000mi

# Reward parameter coefficients
PRETRAIN_REWARD_JOB_WAIT_COEF=0.3
PRETRAIN_REWARD_UTIL_COEF=0.3
PRETRAIN_REWARD_INVALID_COEF=0.4

#TRANSFER_ENV=LargeDC-v0
# leave transfer timesteps blank if not transfer is needed
TRANSFER_TIMESTEPS=
TRANSFER_HOSTS=10
TRANSFER_HOST_PES=10
TRANSFER_HOST_PE_MIPS=10

TRANSFER_JOB_TRACE_FILENAME=100jobs_1lambda_1000mi
TRANSFER_MAX_PES_PER_JOB=1

TRANSFER_REWARD_JOB_WAIT_COEF=0.3
TRANSFER_REWARD_UTIL_COEF=0.3
TRANSFER_REWARD_INVALID_COEF=0.4

# speedup controls the speed in which the jobs are coming
# and when will the last job come.
# in LLNL-Atlas job trace, 1000 means: last job arrives 180 seconds after the first one
# in simulation you have x200 speedup related to wall-clock time
SIMULATION_SPEEDUP=1


# If both pretrain and transfer timesteps are specified, the agent trains on the pratraining
# environment and then gets transfered to the transfer environment